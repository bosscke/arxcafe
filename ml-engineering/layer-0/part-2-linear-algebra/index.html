<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Part 2 — Linear Algebra for ML | Layer 0 | ArxCafe</title>
  <link rel="stylesheet" href="/css/global.css" />
  <link rel="stylesheet" href="/css/ml-engineering.css" />
</head>
<body>
  <a class="home-btn" href="/">← Home</a>

  <div class="ml-shell">
    <div class="ml-topbar">
      <div class="ml-breadcrumbs">
        <a href="/">ArxCafe</a> /
        <a href="/ml-engineering">ML Engineering</a> /
        <a href="/ml-engineering/layer-0">Layer 0</a> /
        <span>Part 2</span>
      </div>
      <span class="ml-pill">Book 0 · Part 2</span>
    </div>

    <section class="ml-panel ml-hero">
      <div class="ml-eyebrow">Layer 0</div>
      <h1>Part 2 — Linear Algebra for ML</h1>
      <div class="ml-subtitle">
        Models do not learn “meaning”. They learn geometry: vectors, distances, similarity, and transformations.
        This part defines the mathematical space where learning happens.
      </div>
    </section>

    <section class="ml-grid">
      <div class="ml-panel">
        <div class="ml-section-title">Purpose</div>
        <div class="ml-body">
          <p>
            Linear algebra is the language of representations. Every dataset becomes a matrix, every model is a function over vectors,
            and many learning algorithms are just repeated linear transforms plus non-linearities.
          </p>
        </div>

        <div class="ml-section-title" style="margin-top: 14px;">Concepts</div>
        <div class="ml-toc">
          <a href="/ml-engineering/layer-0/part-2-linear-algebra/scalars-vectors-feature-spaces">Chapter 0.4 — Scalars, Vectors, and Feature Spaces</a>
          <a href="/ml-engineering/layer-0/part-2-linear-algebra/datasets-as-matrices">Chapter 0.5 — Datasets as Matrices</a>
          <a href="/ml-engineering/layer-0/part-2-linear-algebra/dot-products-similarity">Chapter 0.6 — Dot Products, Projections, and Similarity</a>
          <a href="/ml-engineering/layer-0/part-2-linear-algebra/norms-distances">Chapter 0.7 — Norms, Distances, and Geometry</a>
          <a href="/ml-engineering/layer-0/part-2-linear-algebra/linear-transformations">Chapter 0.8 — Linear Transformations and Model Layers</a>
          <a href="/ml-engineering/layer-0/part-2-linear-algebra/rank-collinearity-numerical-stability">Chapter 0.9 — Rank, Collinearity, and Numerical Stability</a>
          <a href="/ml-engineering/layer-0/part-2-linear-algebra/eigenvalues-pca">Chapter 0.10 — Eigenvalues, PCA, and Dimensionality Reduction</a>
        </div>

        <div class="ml-footer">
          These pages are atomic concept nodes. Later layers will link back here.
        </div>
      </div>

      <aside class="ml-panel">
        <div class="ml-section-title">Where This Appears Later</div>
        <div class="ml-body">
          <ul>
            <li>Layer 3: feature engineering and encodings</li>
            <li>Layer 4: embeddings, representation learning, attention</li>
            <li>Layer 5: large-scale training (matrix ops dominate cost)</li>
          </ul>
        </div>

        <div class="ml-section-title" style="margin-top: 14px;">Navigation</div>
        <div class="ml-toc">
          <a href="/ml-engineering/layer-0">Layer 0 (Book 0)</a>
          <a href="/ml-engineering/layer-0/part-1-learning">← Part 1 — Learning</a>
          <a href="/ml-engineering/layer-0/part-3-probability">Next: Part 3 — Probability Theory →</a>
        </div>
      </aside>
    </section>
  </div>
</body>
</html>
